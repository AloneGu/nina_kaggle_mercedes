{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape train: (4209, 378)\n",
      "Shape test: (4209, 377)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# read datasets\n",
    "train = pd.read_csv('input/train.csv')\n",
    "test = pd.read_csv('input/test.csv')\n",
    "\n",
    "# process columns, apply LabelEncoder to categorical features\n",
    "for c in train.columns:\n",
    "    if train[c].dtype == 'object':\n",
    "        lbl = LabelEncoder() \n",
    "        lbl.fit(list(train[c].values) + list(test[c].values)) \n",
    "        train[c] = lbl.transform(list(train[c].values))\n",
    "        test[c] = lbl.transform(list(test[c].values))\n",
    "\n",
    "# shape        \n",
    "print('Shape train: {}\\nShape test: {}'.format(train.shape, test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape train: (4209, 428)\n",
      "Shape test: (4209, 427)\n",
      "<class 'numpy.ndarray'> (2946, 427)\n"
     ]
    }
   ],
   "source": [
    "# add pca\n",
    "from sklearn.decomposition import PCA, FastICA, TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "n_comp = 10\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=n_comp, random_state=42)\n",
    "pca2_results_train = pca.fit_transform(train.drop([\"y\"], axis=1))\n",
    "pca2_results_test = pca.transform(test)\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=42)\n",
    "ica2_results_train = ica.fit_transform(train.drop([\"y\"], axis=1))\n",
    "ica2_results_test = ica.transform(test)\n",
    "\n",
    "# tSVD\n",
    "tsvd = TruncatedSVD(n_components=n_comp, random_state=420)\n",
    "tsvd_results_train = tsvd.fit_transform(train.drop([\"y\"], axis=1))\n",
    "tsvd_results_test = tsvd.transform(test)\n",
    "\n",
    "# GRP\n",
    "grp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\n",
    "grp_results_train = grp.fit_transform(train.drop([\"y\"], axis=1))\n",
    "grp_results_test = grp.transform(test)\n",
    "\n",
    "# SRP\n",
    "srp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\n",
    "srp_results_train = srp.fit_transform(train.drop([\"y\"], axis=1))\n",
    "srp_results_test = srp.transform(test)\n",
    "\n",
    "# Append decomposition components to datasets\n",
    "for i in range(1, n_comp+1):\n",
    "    train['pca_' + str(i)] = pca2_results_train[:,i-1]\n",
    "    test['pca_' + str(i)] = pca2_results_test[:, i-1]\n",
    "    \n",
    "    train['ica_' + str(i)] = ica2_results_train[:,i-1]\n",
    "    test['ica_' + str(i)] = ica2_results_test[:, i-1]\n",
    "    \n",
    "    train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n",
    "    test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n",
    "    \n",
    "    train['grp_' + str(i)] = grp_results_train[:, i - 1]\n",
    "    test['grp_' + str(i)] = grp_results_test[:, i - 1]\n",
    "\n",
    "    train['srp_' + str(i)] = srp_results_train[:, i - 1]\n",
    "    test['srp_' + str(i)] = srp_results_test[:, i - 1]\n",
    "\n",
    "# shape        \n",
    "print('Shape train: {}\\nShape test: {}'.format(train.shape, test.shape))\n",
    "\n",
    "x_train = train.drop([\"y\"], axis=1).values\n",
    "y_train = train['y'].values\n",
    "# split train data\n",
    "x_train_data,x_test_data,y_train_data,y_test_data = train_test_split(x_train,y_train,test_size=0.3,random_state=23)\n",
    "print(type(x_train_data),x_train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jac/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.652118347623\n",
      "0.550637170345\n",
      "[  91.53730011  105.78723145   78.65650177 ...,   92.05712128  114.97119904\n",
      "   91.95201111] (4209,)\n"
     ]
    }
   ],
   "source": [
    "# train a base xgb regressor\n",
    "import xgboost\n",
    "from sklearn.metrics import r2_score\n",
    "def test_base():\n",
    "    m = xgboost.XGBRegressor()\n",
    "    print(m)\n",
    "    m.fit(x_train_data,y_train_data)\n",
    "    print(r2_score(y_train,m.predict(x_train)))\n",
    "    print(r2_score(y_test_data,m.predict(x_test_data)))\n",
    "    pred = m.predict(test.values)\n",
    "    print(pred,pred.shape)\n",
    "    output = pd.DataFrame({'id': test['ID'].astype(np.int32), 'y': pred})\n",
    "    output.to_csv('result/add_base_xgb.csv',index=False)\n",
    "test_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from copy import copy\n",
    "    \n",
    "class StackedGeneralizer(object):\n",
    "    \"\"\"Base class for stacked generalization classifier models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_models=None, blending_model=None, n_folds=5, verbose=True):\n",
    "        \"\"\"\n",
    "        Stacked Generalizer Classifier\n",
    "\n",
    "        Trains a series of base models using K-fold cross-validation, then combines\n",
    "        the predictions of each model into a set of features that are used to train\n",
    "        a high-level classifier model. \n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        base_models: list of classifier models\n",
    "            Each model must have a .fit and .predict_proba/.predict method a'la\n",
    "            sklearn\n",
    "        blending_model: object\n",
    "            A classifier model used to aggregate the outputs of the trained base\n",
    "            models. Must have a .fit and .predict_proba/.predict method\n",
    "        n_folds: int\n",
    "            The number of K-folds to use in =cross-validated model training\n",
    "        verbose: boolean\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "\n",
    "        from sklearn.datasets import load_digits\n",
    "        from stacked_generalizer import StackedGeneralizer\n",
    "        from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        import numpy as np\n",
    "\n",
    "        logger = Logger('test_stacked_generalizer')\n",
    "\n",
    "        VERBOSE = True\n",
    "        N_FOLDS = 5\n",
    "        \n",
    "        # load data and shuffle observations\n",
    "        data = load_digits()\n",
    "\n",
    "        X = data.data\n",
    "        y = data.target\n",
    "\n",
    "        shuffle_idx = np.random.permutation(y.size)\n",
    "\n",
    "        X = X[shuffle_idx]\n",
    "        y = y[shuffle_idx]\n",
    "\n",
    "        # hold out 20 percent of data for testing accuracy\n",
    "        n_train = round(X.shape[0]*.8)\n",
    "\n",
    "        # define base models\n",
    "        base_models = [RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "                       RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "                       ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini')]\n",
    "\n",
    "        # define blending model\n",
    "        blending_model = LogisticRegression()\n",
    "\n",
    "        # initialize multi-stage model\n",
    "        sg = StackedGeneralizer(base_models, blending_model, \n",
    "                                n_folds=N_FOLDS, verbose=VERBOSE)\n",
    "\n",
    "        # fit model\n",
    "        sg.fit(X[:n_train],y[:n_train])\n",
    "\n",
    "        # test accuracy\n",
    "        pred = sg.predict(X[n_train:])\n",
    "        pred_classes = [np.argmax(p) for p in pred]\n",
    "\n",
    "        _ = sg.evaluate(y[n_train:], pred_classes)\n",
    "\n",
    "                     precision    recall  f1-score   support\n",
    "\n",
    "                  0       0.97      1.00      0.99        33\n",
    "                  1       0.97      1.00      0.99        38\n",
    "                  2       1.00      1.00      1.00        42\n",
    "                  3       1.00      0.98      0.99        41\n",
    "                  4       0.97      0.94      0.95        32\n",
    "                  5       0.95      0.98      0.96        41\n",
    "                  6       1.00      0.95      0.97        37\n",
    "                  7       0.94      0.97      0.96        34\n",
    "                  8       0.94      0.94      0.94        34\n",
    "                  9       0.96      0.96      0.96        27\n",
    "\n",
    "        avg / total       0.97      0.97      0.97       359\n",
    "        \"\"\"\n",
    "        self.base_models = base_models\n",
    "        self.blending_model = blending_model\n",
    "        self.n_folds = n_folds\n",
    "        self.verbose = verbose\n",
    "        self.base_models_cv = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.fit_base_models(X, y)\n",
    "        print('fit base models done')\n",
    "        self.fit_blending_model(X, y)\n",
    "        print('fit blend model done')\n",
    "    \n",
    "    def fit_base_models(self, X, y):\n",
    "        if self.verbose:\n",
    "            print('Fitting Base Models...')\n",
    "\n",
    "        kf = list(KFold(y.shape[0], self.n_folds))\n",
    "\n",
    "        for i, model in enumerate(self.base_models):    \n",
    "            for j, (train_idx, test_idx) in enumerate(kf):\n",
    "                if self.verbose:\n",
    "                    print('Fold %d' % (j + 1))\n",
    "                # print(X.shape,min(train_idx),max(train_idx),len(train_idx),type(X))\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # add trained model to list of CV'd models\n",
    "                self.base_models_cv.append(copy(model))\n",
    "        \n",
    "\n",
    "    def fit_blending_model(self,X, y):\n",
    "        if self.verbose:\n",
    "            model_name = \"%s\" % self.blending_model.__repr__()\n",
    "            print('Fitting Blending Model:\\n%s' % model_name)\n",
    "\n",
    "        predictions = []\n",
    "        print('model cnts',len(self.base_models_cv))\n",
    "        for m in self.base_models_cv:\n",
    "            base_res = m.predict(X)\n",
    "            print(base_res.shape)\n",
    "            predictions.append(base_res)\n",
    "\n",
    "        # transpose base model output as blend model input\n",
    "        blend_x = np.array(predictions).transpose()\n",
    "        print('blend_shape',blend_x.shape)\n",
    "        self.blending_model.fit(blend_x,y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # perform model averaging to get predictions\n",
    "        predictions = []\n",
    "        for m in self.base_models_cv:\n",
    "            predictions.append(m.predict(X))\n",
    "\n",
    "        # transpose base model output as blend model input\n",
    "        blend_x = np.array(predictions).transpose()\n",
    "        return self.blending_model.predict(blend_x)\n",
    "\n",
    "    def evaluate(self, y, y_pred):\n",
    "        print(classification_report(y, y_pred))\n",
    "        print('Confusion Matrix:')\n",
    "        print(confusion_matrix(y, y_pred))\n",
    "        return accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Base Models...\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "fit base models done\n",
      "Fitting Blending Model:\n",
      "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "model cnts 9\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "blend_shape (2946, 9)\n",
      "fit blend model done\n",
      "0.832992017236\n",
      "0.457162856673\n",
      "[  81.19771576  100.81538391   80.2059021  ...,   97.37402344  112.05316162\n",
      "   94.9573288 ] (4209,)\n"
     ]
    }
   ],
   "source": [
    "# try stack generalization\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "\n",
    "def stack_test():\n",
    "    VERBOSE = True\n",
    "    N_FOLDS = 3\n",
    "\n",
    "    base_models = [#RandomForestRegressor(n_estimators=20),\n",
    "                   #RandomForestRegressor(n_estimators=50), \n",
    "                   #ExtraTreesRegressor(),\n",
    "                   #AdaBoostRegressor(),\n",
    "                   #Ridge(),\n",
    "                   #LinearRegression(),\n",
    "                   xgboost.XGBRegressor(n_estimators=20),\n",
    "                   xgboost.XGBRegressor(subsample=0.5),\n",
    "                   xgboost.XGBRegressor(max_depth=10)\n",
    "                  ]\n",
    "    blending_model = xgboost.XGBRegressor()\n",
    "    \n",
    "    # initialize multi-stage model\n",
    "    sg = StackedGeneralizer(base_models, blending_model, n_folds=N_FOLDS, verbose=VERBOSE)\n",
    "    sg.fit(x_train_data,y_train_data)\n",
    "    \n",
    "    print(r2_score(y_train,sg.predict(x_train).flatten()))\n",
    "    print(r2_score(y_test_data,sg.predict(x_test_data).flatten()))\n",
    "    pred = sg.predict(test.values).flatten()\n",
    "    print(pred,pred.shape)\n",
    "    output = pd.DataFrame({'id': test['ID'].astype(np.int32), 'y': pred})\n",
    "    output.to_csv('result/add_stack_sklearn.csv',index=False)\n",
    "\n",
    "stack_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Base Models...\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "fit base models done\n",
      "Fitting Blending Model:\n",
      "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "model cnts 27\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "blend_shape (2946, 27)\n",
      "fit blend model done\n",
      "0.837295571899\n",
      "0.438299336661\n",
      "[  80.12405396  117.0450058    78.28937531 ...,   96.78491211  110.16893005\n",
      "   92.93627167] (4209,)\n"
     ]
    }
   ],
   "source": [
    "def stack_test():\n",
    "    VERBOSE = True\n",
    "    N_FOLDS = 3\n",
    "\n",
    "    base_models = [RandomForestRegressor(n_estimators=20),\n",
    "                   RandomForestRegressor(n_estimators=50), \n",
    "                   ExtraTreesRegressor(),\n",
    "                   AdaBoostRegressor(),\n",
    "                   Ridge(),\n",
    "                   LinearRegression(),\n",
    "                   xgboost.XGBRegressor(n_estimators=20),\n",
    "                   xgboost.XGBRegressor(subsample=0.5),\n",
    "                   xgboost.XGBRegressor(max_depth=10)\n",
    "                  ]\n",
    "    blending_model = xgboost.XGBRegressor()\n",
    "    \n",
    "    # initialize multi-stage model\n",
    "    sg = StackedGeneralizer(base_models, blending_model, n_folds=N_FOLDS, verbose=VERBOSE)\n",
    "    sg.fit(x_train_data,y_train_data)\n",
    "    \n",
    "    print(r2_score(y_train,sg.predict(x_train).flatten()))\n",
    "    print(r2_score(y_test_data,sg.predict(x_test_data).flatten()))\n",
    "    pred = sg.predict(test.values).flatten()\n",
    "    print(pred,pred.shape)\n",
    "    output = pd.DataFrame({'id': test['ID'].astype(np.int32), 'y': pred})\n",
    "    output.to_csv('result/add_stack_more_model_sklearn.csv',index=False)\n",
    "\n",
    "stack_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Base Models...\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "fit base models done\n",
      "Fitting Blending Model:\n",
      "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "model cnts 27\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "blend_shape (2946, 27)\n",
      "fit blend model done\n",
      "0.838056654323\n",
      "0.443171631076\n",
      "[  79.36005402  122.00016785   77.81624603 ...,  102.04833984  109.56017303\n",
      "   91.28230286] (4209,)\n"
     ]
    }
   ],
   "source": [
    "def stack_test():\n",
    "    VERBOSE = True\n",
    "    N_FOLDS = 3\n",
    "\n",
    "    base_models = [RandomForestRegressor(n_estimators=20),\n",
    "                   RandomForestRegressor(n_estimators=50), \n",
    "                   ExtraTreesRegressor(),\n",
    "                   AdaBoostRegressor(),\n",
    "                   Ridge(),\n",
    "                   LinearRegression(),\n",
    "                   xgboost.XGBRegressor(n_estimators=20),\n",
    "                   xgboost.XGBRegressor(subsample=0.5),\n",
    "                   xgboost.XGBRegressor(max_depth=10)\n",
    "                  ]\n",
    "    blending_model = xgboost.XGBRegressor()\n",
    "    \n",
    "    # initialize multi-stage model\n",
    "    sg = StackedGeneralizer(base_models, blending_model, n_folds=N_FOLDS, verbose=VERBOSE)\n",
    "    sg.fit(x_train_data,y_train_data)\n",
    "    \n",
    "    print(r2_score(y_train,sg.predict(x_train).flatten()))\n",
    "    print(r2_score(y_test_data,sg.predict(x_test_data).flatten()))\n",
    "    pred = sg.predict(test.values).flatten()\n",
    "    print(pred,pred.shape)\n",
    "    output = pd.DataFrame({'id': test['ID'].astype(np.int32), 'y': pred})\n",
    "    output.to_csv('result/add_stack_more_model_all_data_sklearn.csv',index=False)\n",
    "\n",
    "stack_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
