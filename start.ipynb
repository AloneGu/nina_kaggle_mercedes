{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape train: (4209, 378)\n",
      "Shape test: (4209, 377)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# read datasets\n",
    "train = pd.read_csv('input/train.csv')\n",
    "test = pd.read_csv('input/test.csv')\n",
    "\n",
    "# process columns, apply LabelEncoder to categorical features\n",
    "for c in train.columns:\n",
    "    if train[c].dtype == 'object':\n",
    "        lbl = LabelEncoder() \n",
    "        lbl.fit(list(train[c].values) + list(test[c].values)) \n",
    "        train[c] = lbl.transform(list(train[c].values))\n",
    "        test[c] = lbl.transform(list(test[c].values))\n",
    "\n",
    "# shape        \n",
    "print('Shape train: {}\\nShape test: {}'.format(train.shape, test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape train: (4209, 398)\n",
      "Shape test: (4209, 397)\n",
      "<class 'numpy.ndarray'> (2946, 397)\n"
     ]
    }
   ],
   "source": [
    "# add pca\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.model_selection import train_test_split\n",
    "n_comp = 10\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=n_comp, random_state=42)\n",
    "pca2_results_train = pca.fit_transform(train.drop([\"y\"], axis=1))\n",
    "pca2_results_test = pca.transform(test)\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=42)\n",
    "ica2_results_train = ica.fit_transform(train.drop([\"y\"], axis=1))\n",
    "ica2_results_test = ica.transform(test)\n",
    "\n",
    "# Append decomposition components to datasets\n",
    "for i in range(1, n_comp+1):\n",
    "    train['pca_' + str(i)] = pca2_results_train[:,i-1]\n",
    "    test['pca_' + str(i)] = pca2_results_test[:, i-1]\n",
    "    \n",
    "    train['ica_' + str(i)] = ica2_results_train[:,i-1]\n",
    "    test['ica_' + str(i)] = ica2_results_test[:, i-1]\n",
    "\n",
    "# shape        \n",
    "print('Shape train: {}\\nShape test: {}'.format(train.shape, test.shape))\n",
    "\n",
    "x_train = train.drop([\"y\"], axis=1).values\n",
    "y_train = train['y'].values\n",
    "# split train data\n",
    "x_train_data,x_test_data,y_train_data,y_test_data = train_test_split(x_train,y_train,test_size=0.3,random_state=23)\n",
    "print(type(x_train_data),x_train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "0.649680059576\n",
      "0.551227482594\n",
      "[  97.22125244  113.28610229   78.88557434 ...,   92.72019196  110.1289444\n",
      "   92.69352722] (4209,)\n"
     ]
    }
   ],
   "source": [
    "# train a base xgb regressor\n",
    "import xgboost\n",
    "from sklearn.metrics import r2_score\n",
    "def test_base():\n",
    "    m = xgboost.XGBRegressor()\n",
    "    print(m)\n",
    "    m.fit(x_train_data,y_train_data)\n",
    "    print(r2_score(y_train,m.predict(x_train)))\n",
    "    print(r2_score(y_test_data,m.predict(x_test_data)))\n",
    "    pred = m.predict(test.values)\n",
    "    print(pred,pred.shape)\n",
    "    output = pd.DataFrame({'id': test['ID'].astype(np.int32), 'y': pred})\n",
    "    output.to_csv('result/base_xgb.csv',index=False)\n",
    "test_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from copy import copy\n",
    "    \n",
    "class StackedGeneralizer(object):\n",
    "    \"\"\"Base class for stacked generalization classifier models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_models=None, blending_model=None, n_folds=5, verbose=True):\n",
    "        \"\"\"\n",
    "        Stacked Generalizer Classifier\n",
    "\n",
    "        Trains a series of base models using K-fold cross-validation, then combines\n",
    "        the predictions of each model into a set of features that are used to train\n",
    "        a high-level classifier model. \n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        base_models: list of classifier models\n",
    "            Each model must have a .fit and .predict_proba/.predict method a'la\n",
    "            sklearn\n",
    "        blending_model: object\n",
    "            A classifier model used to aggregate the outputs of the trained base\n",
    "            models. Must have a .fit and .predict_proba/.predict method\n",
    "        n_folds: int\n",
    "            The number of K-folds to use in =cross-validated model training\n",
    "        verbose: boolean\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "\n",
    "        from sklearn.datasets import load_digits\n",
    "        from stacked_generalizer import StackedGeneralizer\n",
    "        from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        import numpy as np\n",
    "\n",
    "        logger = Logger('test_stacked_generalizer')\n",
    "\n",
    "        VERBOSE = True\n",
    "        N_FOLDS = 5\n",
    "        \n",
    "        # load data and shuffle observations\n",
    "        data = load_digits()\n",
    "\n",
    "        X = data.data\n",
    "        y = data.target\n",
    "\n",
    "        shuffle_idx = np.random.permutation(y.size)\n",
    "\n",
    "        X = X[shuffle_idx]\n",
    "        y = y[shuffle_idx]\n",
    "\n",
    "        # hold out 20 percent of data for testing accuracy\n",
    "        n_train = round(X.shape[0]*.8)\n",
    "\n",
    "        # define base models\n",
    "        base_models = [RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "                       RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "                       ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini')]\n",
    "\n",
    "        # define blending model\n",
    "        blending_model = LogisticRegression()\n",
    "\n",
    "        # initialize multi-stage model\n",
    "        sg = StackedGeneralizer(base_models, blending_model, \n",
    "                                n_folds=N_FOLDS, verbose=VERBOSE)\n",
    "\n",
    "        # fit model\n",
    "        sg.fit(X[:n_train],y[:n_train])\n",
    "\n",
    "        # test accuracy\n",
    "        pred = sg.predict(X[n_train:])\n",
    "        pred_classes = [np.argmax(p) for p in pred]\n",
    "\n",
    "        _ = sg.evaluate(y[n_train:], pred_classes)\n",
    "\n",
    "                     precision    recall  f1-score   support\n",
    "\n",
    "                  0       0.97      1.00      0.99        33\n",
    "                  1       0.97      1.00      0.99        38\n",
    "                  2       1.00      1.00      1.00        42\n",
    "                  3       1.00      0.98      0.99        41\n",
    "                  4       0.97      0.94      0.95        32\n",
    "                  5       0.95      0.98      0.96        41\n",
    "                  6       1.00      0.95      0.97        37\n",
    "                  7       0.94      0.97      0.96        34\n",
    "                  8       0.94      0.94      0.94        34\n",
    "                  9       0.96      0.96      0.96        27\n",
    "\n",
    "        avg / total       0.97      0.97      0.97       359\n",
    "        \"\"\"\n",
    "        self.base_models = base_models\n",
    "        self.blending_model = blending_model\n",
    "        self.n_folds = n_folds\n",
    "        self.verbose = verbose\n",
    "        self.base_models_cv = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.fit_base_models(X, y)\n",
    "        print('fit base models done')\n",
    "        self.fit_blending_model(X, y)\n",
    "        print('fit blend model done')\n",
    "    \n",
    "    def fit_base_models(self, X, y):\n",
    "        if self.verbose:\n",
    "            print('Fitting Base Models...')\n",
    "\n",
    "        kf = list(KFold(y.shape[0], self.n_folds))\n",
    "\n",
    "        for i, model in enumerate(self.base_models):    \n",
    "            for j, (train_idx, test_idx) in enumerate(kf):\n",
    "                if self.verbose:\n",
    "                    print('Fold %d' % (j + 1))\n",
    "                # print(X.shape,min(train_idx),max(train_idx),len(train_idx),type(X))\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # add trained model to list of CV'd models\n",
    "                self.base_models_cv.append(copy(model))\n",
    "        \n",
    "\n",
    "    def fit_blending_model(self,X, y):\n",
    "        if self.verbose:\n",
    "            model_name = \"%s\" % self.blending_model.__repr__()\n",
    "            print('Fitting Blending Model:\\n%s' % model_name)\n",
    "\n",
    "        predictions = []\n",
    "        print('model cnts',len(self.base_models_cv))\n",
    "        for m in self.base_models_cv:\n",
    "            base_res = m.predict(X)\n",
    "            print(base_res.shape)\n",
    "            predictions.append(base_res)\n",
    "\n",
    "        # transpose base model output as blend model input\n",
    "        blend_x = np.array(predictions).transpose()\n",
    "        print('blend_shape',blend_x.shape)\n",
    "        self.blending_model.fit(blend_x,y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # perform model averaging to get predictions\n",
    "        predictions = []\n",
    "        for m in self.base_models_cv:\n",
    "            predictions.append(m.predict(X))\n",
    "\n",
    "        # transpose base model output as blend model input\n",
    "        blend_x = np.array(predictions).transpose()\n",
    "        return self.blending_model.predict(blend_x)\n",
    "\n",
    "    def evaluate(self, y, y_pred):\n",
    "        print(classification_report(y, y_pred))\n",
    "        print('Confusion Matrix:')\n",
    "        print(confusion_matrix(y, y_pred))\n",
    "        return accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Base Models...\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "fit base models done\n",
      "Fitting Blending Model:\n",
      "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "model cnts 9\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "blend_shape (2946, 9)\n",
      "fit blend model done\n",
      "0.820923950068\n",
      "0.425362425146\n",
      "[  79.63766479   96.60379791   79.08563995 ...,   89.90055084  107.39347839\n",
      "   90.8680191 ] (4209,)\n"
     ]
    }
   ],
   "source": [
    "# try stack generalization\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "\n",
    "def stack_test():\n",
    "    VERBOSE = True\n",
    "    N_FOLDS = 3\n",
    "\n",
    "    base_models = [#RandomForestRegressor(n_estimators=20),\n",
    "                   #RandomForestRegressor(n_estimators=50), \n",
    "                   #ExtraTreesRegressor(),\n",
    "                   #AdaBoostRegressor(),\n",
    "                   #Ridge(),\n",
    "                   #LinearRegression(),\n",
    "                   xgboost.XGBRegressor(n_estimators=20),\n",
    "                   xgboost.XGBRegressor(subsample=0.5),\n",
    "                   xgboost.XGBRegressor(max_depth=10)\n",
    "                  ]\n",
    "    blending_model = xgboost.XGBRegressor()\n",
    "    \n",
    "    # initialize multi-stage model\n",
    "    sg = StackedGeneralizer(base_models, blending_model, n_folds=N_FOLDS, verbose=VERBOSE)\n",
    "    sg.fit(x_train_data,y_train_data)\n",
    "    \n",
    "    print(r2_score(y_train,sg.predict(x_train).flatten()))\n",
    "    print(r2_score(y_test_data,sg.predict(x_test_data).flatten()))\n",
    "    pred = sg.predict(test.values).flatten()\n",
    "    print(pred,pred.shape)\n",
    "    output = pd.DataFrame({'id': test['ID'].astype(np.int32), 'y': pred})\n",
    "    output.to_csv('result/stack_sklearn.csv',index=False)\n",
    "\n",
    "stack_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Base Models...\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "fit base models done\n",
      "Fitting Blending Model:\n",
      "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "model cnts 27\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "(2946,)\n",
      "blend_shape (2946, 27)\n",
      "fit blend model done\n",
      "0.837092919043\n",
      "0.442121590654\n",
      "[  78.40608978  109.59427643   77.67073822 ...,   94.47338104  108.69316864\n",
      "   91.86972809] (4209,)\n"
     ]
    }
   ],
   "source": [
    "def stack_test():\n",
    "    VERBOSE = True\n",
    "    N_FOLDS = 3\n",
    "\n",
    "    base_models = [RandomForestRegressor(n_estimators=20),\n",
    "                   RandomForestRegressor(n_estimators=50), \n",
    "                   ExtraTreesRegressor(),\n",
    "                   AdaBoostRegressor(),\n",
    "                   Ridge(),\n",
    "                   LinearRegression(),\n",
    "                   xgboost.XGBRegressor(n_estimators=20),\n",
    "                   xgboost.XGBRegressor(subsample=0.5),\n",
    "                   xgboost.XGBRegressor(max_depth=10)\n",
    "                  ]\n",
    "    blending_model = xgboost.XGBRegressor()\n",
    "    \n",
    "    # initialize multi-stage model\n",
    "    sg = StackedGeneralizer(base_models, blending_model, n_folds=N_FOLDS, verbose=VERBOSE)\n",
    "    sg.fit(x_train_data,y_train_data)\n",
    "    \n",
    "    print(r2_score(y_train,sg.predict(x_train).flatten()))\n",
    "    print(r2_score(y_test_data,sg.predict(x_test_data).flatten()))\n",
    "    pred = sg.predict(test.values).flatten()\n",
    "    print(pred,pred.shape)\n",
    "    output = pd.DataFrame({'id': test['ID'].astype(np.int32), 'y': pred})\n",
    "    output.to_csv('result/stack_more_model_sklearn.csv',index=False)\n",
    "\n",
    "stack_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Base Models...\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "fit base models done\n",
      "Fitting Blending Model:\n",
      "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "model cnts 27\n",
      "(4209,)\n",
      "(4209,)\n",
      "(4209,)\n",
      "(4209,)\n",
      "(4209,)\n",
      "(4209,)\n",
      "(4209,)\n",
      "(4209,)\n",
      "(4209,)\n",
      "(4209,)\n",
      "(4209,)\n",
      "(4209,)\n",
      "(4209,)\n",
      "(4209,)\n",
      "(4209,)\n",
      "(4209,)\n",
      "(4209,)\n",
      "(4209,)\n",
      "(4209,)\n",
      "(4209,)\n",
      "(4209,)\n",
      "(4209,)\n",
      "(4209,)\n",
      "(4209,)\n",
      "(4209,)\n",
      "(4209,)\n",
      "(4209,)\n",
      "blend_shape (4209, 27)\n",
      "fit blend model done\n",
      "0.983794085043\n",
      "0.981403668587\n",
      "[  78.36032104  118.05523682   77.24809265 ...,   92.7756958   108.74485779\n",
      "   91.37864685] (4209,)\n"
     ]
    }
   ],
   "source": [
    "def stack_test():\n",
    "    VERBOSE = True\n",
    "    N_FOLDS = 3\n",
    "\n",
    "    base_models = [RandomForestRegressor(n_estimators=20),\n",
    "                   RandomForestRegressor(n_estimators=50), \n",
    "                   ExtraTreesRegressor(),\n",
    "                   AdaBoostRegressor(),\n",
    "                   Ridge(),\n",
    "                   LinearRegression(),\n",
    "                   xgboost.XGBRegressor(n_estimators=20),\n",
    "                   xgboost.XGBRegressor(subsample=0.5),\n",
    "                   xgboost.XGBRegressor(max_depth=10)\n",
    "                  ]\n",
    "    blending_model = xgboost.XGBRegressor()\n",
    "    \n",
    "    # initialize multi-stage model\n",
    "    sg = StackedGeneralizer(base_models, blending_model, n_folds=N_FOLDS, verbose=VERBOSE)\n",
    "    sg.fit(x_train,y_train)\n",
    "    \n",
    "    print(r2_score(y_train,sg.predict(x_train).flatten()))\n",
    "    print(r2_score(y_test_data,sg.predict(x_test_data).flatten()))\n",
    "    pred = sg.predict(test.values).flatten()\n",
    "    print(pred,pred.shape)\n",
    "    output = pd.DataFrame({'id': test['ID'].astype(np.int32), 'y': pred})\n",
    "    output.to_csv('result/stack_more_model_all_data_sklearn.csv',index=False)\n",
    "\n",
    "stack_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
